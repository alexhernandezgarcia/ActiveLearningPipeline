device : "cpu"
debug : True
al:
  n_iter: 3
  queries_per_it: 100

path :
  data_oracle : "/home/mila/n/nikita.saxena/original/ActiveLearningPipeline/storage/data_oracle/database.npy"
  model_proxy : "/home/mila/n/nikita.saxena/original/ActiveLearningPipeline/storage/model_proxy/proxy"
  model_gfn : "/home/mila/n/nikita.saxena/original/ActiveLearningPipeline/storage/model_gfn/gfn"
  model_oracle_MLP : "/home/mila/n/nikita.saxena/original/ActiveLearningPipeline/storage/model_oracle/oracle_MLP"

env:
  main : "aptamers"
  dict_size : 4
  min_len : 10
  max_len : 20
  min_word_len : 1
  max_word_len : 1
  total_fidelities : 3

oracle:
  main : "toy"
  init_dataset:
    seed : 0
    init_len : 100
  
proxy:
  model : "mlp" #the architecture is specified in ProxyMLP class
  training:
    eps : 1.0e-4
    dropout : 0.1
    max_epochs : 200
    training_batch : 10
    history : 20
  data:
    shuffle: True
    seed : 10

gflownet:
  policy_model : "mlp"
  loss : 
    function: "flowmatch"
    flowmatch_eps: 1.0e-10
  view_progress : True
  sampling:
    temperature : 1
    seed : 1
    random_action_prob : 0.1
  training:
    pct_batch_empirical : 0.1
    batch_size : 16
    training_steps : 1000
    clip_grad_norm : 10
    dropout: 0.1
    opt : "adam"
    adam_beta1 : 0.9
    adam_beta2 : 0.99
    momentum : 0.8
    learning_rate : 1.0e-3
    lr_decay_period : 1.0e+6
    lr_decay_gamma : 0.75
    ttsr : 3

acquisition:
  main: "MES"
  

  

    


